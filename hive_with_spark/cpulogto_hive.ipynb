{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\"\"\"\n",
    "\n",
    "@Author: Rikesh Chhetri\n",
    "\n",
    "@Date: 2021-08-28\n",
    "\n",
    "@Last Modified by: Rikesh Chhetri\n",
    "\n",
    "@Last Modified time: 2021-08-28 07:03:30\n",
    "\n",
    "@Title : Program to insert a cpu log data.csv file from hdfs and using spark convert it into dataframe and putting it into hive table\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# creating spark sesion with hive support enable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from loghandler import logger\n",
    "try:\n",
    "    appName = \"PySpark MySQL Example - via mysql.connector\"\n",
    "    master = \"local\"\n",
    "    spark = SparkSession.builder.master(master).appName(appName).enableHiveSupport().getOrCreate()\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# creating dataframe of csv file using spark and selecting only 4 columns from csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    df = spark.read.csv(\"hdfs://localhost:9000/csv_data/*.csv\",header=True)\n",
    "    df2 = df.select(\"user_name\",\"DateTime\",\"keyboard\",\"mouse\")\n",
    "    df2.createOrReplaceTempView('myview')\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now Adding these dataframe into hive table using different method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    # Save df to a new table in Hive we just need database table name we can give anything\n",
    "    df2.write.mode(\"overwrite\").saveAsTable(\"test.test_data\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Method 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### loading spark dataframe by creating view and inserting it into hive "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    # here test is a database name and just give anything for table name it will create automatically\n",
    "    spark.sql(\"create table test.test2 as select * from myview\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Showing The hive tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    \n",
    "    dfs = spark.sql(\"use test\")\n",
    "    dfs1 = spark.sql(\"show tables\")\n",
    "    dfs1.show()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}